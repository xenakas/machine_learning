{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 4\n",
    "\n",
    "## Реализация градиентного бустинга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках этой задачи нужно написать градиентный бустинг над решающими деревьями в задаче классификации. В качестве функции потерь предлагается взять **log loss**. Про рего можно прочитать подробнее здесь: https://scikit-learn.org/stable/modules/model_evaluation.html#log-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$y_i$ это правильный ответ (0 или 1), $\\hat{y}_i$ это ваше предскзаание\n",
    "\n",
    "Может показаться, что надо максимизировать функцию $L(\\hat{y}, y) = \\sum_{i=1}^n y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)$, где $y_i$\n",
    "\n",
    "Да, но нет. Лучше максимизировать функцию $L(\\hat{y}, y) = \\sum_{i=1}^n y_i \\log(f(\\hat{y}_i)) + (1 - y_i) \\log(1 - f(\\hat{y}_i))$, где $f(x) = \\frac{1}{1 + e^{-x}}$. Благодаря этому у вас не будет ограничений на принимаеммые значения для $\\hat{y}_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "\n",
    "Напишите вычисление производной f(x), обычно её называют **сигмоида**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "def der_sigmoid(x):\n",
    "    return np.exp(-x) / (1 + np.exp(-x))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der_sigmoid(0) == 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der_sigmoid(np.array([0, 0])) == np.array([0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der_sigmoid(np.log(3)) == 0.1875"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Значение для формы:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9212\n"
     ]
    }
   ],
   "source": [
    "print(round(der_sigmoid(np.array([-10, 4.1, -1, 2])).sum() + sigmoid(0.42), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошо, теперь мы умеем считать производную функции f, но надо найти производную log loss-а по $\\hat{y}$ в первом варианте \n",
    "\n",
    "$$y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "\n",
    "Напишите вычисление производной log loss-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_log_loss(y_hat, y_true):\n",
    "    \"\"\"\n",
    "    0 < y_hat < 1\n",
    "    \"\"\"\n",
    "    assert np.all(y_hat < 1) and np.all(y_hat > 0)\n",
    "    \n",
    "    return (y_true - y_hat) / (y_hat - y_hat**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der_log_loss(0.5, 0) == -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der_log_loss(0.5, 1) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(der_log_loss(np.array([0.8, 0.8]), np.array([1, 1])), 2) == np.array([1.25, 1.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Значение для формы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.82\n"
     ]
    }
   ],
   "source": [
    "print(round(-sum(der_log_loss((x + 1) / 100., x % 2) for x in range(99)), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично теперь мы можем воспользоваться производной сложной функции и получить вычисление градиента формулы по второму варианту:\n",
    "\n",
    "$$L(\\hat{y}, y) = \\sum_{i=1}^n y_i \\log(f(\\hat{y}_i)) + (1 - y_i) \\log(1 - f(\\hat{y}_i))$$ $$f(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient(y_hat, y_true):\n",
    "    return der_log_loss(sigmoid(y_hat), y_true) * der_sigmoid(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем написать код градиентного бустинга для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "\n",
    "Допишите класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator # чтобы поддержать интерфейс sklearn\n",
    "from sklearn.tree import DecisionTreeRegressor # для обучения на каждой итерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGB(BaseEstimator):\n",
    "    def __init__(self, tree_params_dict, iters=100, tau=1e-1):\n",
    "        \"\"\"\n",
    "        tree_params_dict - словарь параметров, которые надо использовать при обучении дерева на итерации\n",
    "        iters - количество итераций\n",
    "        tau - коэффициент перед предсказаниями деревьев на каждой итерации\n",
    "        \"\"\"\n",
    "        self.tree_params_dict = tree_params_dict\n",
    "        self.iters = iters\n",
    "        self.tau = tau\n",
    "        \n",
    "    def fit(self, X_data, y_data):\n",
    "        \n",
    "        self.estimators = []\n",
    "        curr_pred = 0\n",
    "        \n",
    "        for iter_num in range(self.iters):\n",
    "            \n",
    "            # Нужно найти градиент функции потерь по предсказниям в точке curr_pred\n",
    "            grad = calc_gradient(curr_pred, y_data)\n",
    "            \n",
    "            # Мы максимизируем, поэтому надо обучить DecisionTreeRegressor с параметрами \n",
    "            # tree_params_dict по X_data предсказывать grad\n",
    "            algo = DecisionTreeRegressor(**self.tree_params_dict).fit(X_data, grad)\n",
    "            \n",
    "            self.estimators.append(algo)\n",
    "            \n",
    "            # все предсказания домножаются на tau и обновляется переменная curr_pred\n",
    "            curr_pred += self.tau * algo.predict(X_data)\n",
    "            \n",
    "            \n",
    "        #return self.estimators\n",
    "       \n",
    "    \n",
    "    def predict(self, X_data):\n",
    "        \n",
    "        # изначально все предскзания нули\n",
    "        res = np.zeros(X_data.shape[0])\n",
    "        \n",
    "        for estimator in self.estimators:\n",
    "            # нужно сложить все предсказания деревьев с весом self.tau\n",
    "            res += self.tau * estimator.predict(X_data)\n",
    "            \n",
    "        return (res > 0).astype(int)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества полученного класса (в самом низу код для формы)\n",
    "\n",
    "Можете поиграться с параметрами, посмотрим, у кого самое лучшее качество получится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для оценки качества\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# для генерации датасетов\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# для сравнения\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = make_classification(n_samples=1000, n_features=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SimpleGB(\n",
    "    tree_params_dict={\n",
    "        'max_depth':4\n",
    "    },\n",
    "    iters=1000,\n",
    "    tau = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.922"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(algo, X_data, y_data, cv=5, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8540288007200181"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(DecisionTreeClassifier(), X_data, y_data, cv=5, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8969841246031149"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(XGBClassifier(), X_data, y_data, cv=5, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8560087002175054"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(LogisticRegression(), X_data, y_data, cv=5, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Значение для формы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(round(np.mean(cross_val_score(SimpleGB(\n",
    "    tree_params_dict={\n",
    "        'max_depth': 4\n",
    "    },\n",
    "    iters=1000,\n",
    "    tau = 0.01\n",
    "), X_data, y_data, cv=StratifiedKFold(4, random_state=42), scoring='accuracy')), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
