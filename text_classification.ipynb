{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейные модели в классификации текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Image\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посмотрим на данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример элемента выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features\n",
      "\n",
      " From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target\n",
      "\n",
      " 7\n"
     ]
    }
   ],
   "source": [
    "print('Features\\n\\n', dataset['data'][0])\n",
    "print('Target\\n\\n', dataset.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train и test выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём 2 темы из одного раздела, возможно, их будет сложно отличить друг от друга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'rec.sport.baseball',\n",
    "    'rec.sport.hockey'\n",
    "]\n",
    "train_data = fetch_20newsgroups(subset='train',\n",
    "                                categories=categories,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "test_data = fetch_20newsgroups(subset='test',\n",
    "                               categories=categories,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# examples in train 1197\n",
      "# examples in test 796\n"
     ]
    }
   ],
   "source": [
    "print('# examples in train', len(train_data.data))\n",
    "print('# examples in test', len(test_data.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приведем данные к виду, с которым можно работать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для векторизации текстов воспользуемся CountVectorizer, он представляет документ как мешок слов. Можно всячески варировать извлечение признаков (убирать редкие слова, убирать частые слова, убирать слова общей лексики, брать биграмы и т.д.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=3, ngram_range=(1, 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с эффективным представлением матриц \"Compressed Sparse Row Format\" (CSR). Почитать о формате можно здесь: https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_feature_matrix = count_vectorizer.fit_transform(train_data.data)\n",
    "sparse_feature_matrix = dense_feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense matrix shape (142691,)\n",
      "Sparse matrix shape (1197, 13338)\n"
     ]
    }
   ],
   "source": [
    "print('Dense matrix shape', dense_feature_matrix.data.shape)\n",
    "print('Sparse matrix shape', sparse_feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.53 ms, sys: 64 µs, total: 1.59 ms\n",
      "Wall time: 822 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228812"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dense_feature_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15 ms, sys: 4.15 ms, total: 19.2 ms\n",
      "Wall time: 18.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228812"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sparse_feature_matrix.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Словарь токен-слово"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем обратный словарь, в котором каждому токену (номеру) будет сопоставлено слово из оригинального словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_2_words = {\n",
    "    v: k\n",
    "    for k, v in count_vectorizer.vocabulary_.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начнем обучать линейные модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим логистическую регрессию предсказывать тему документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = LogisticRegression()\n",
    "algo.fit(dense_feature_matrix, train_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слова с наименьшим отрицательным весом являются характерными словами класса 0\n",
    "\n",
    "Слова с наибольшим положительным весом являются характерными словами класса 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseball\n",
      "runs, jewish, baseball, braves, stadium, cubs, day, hall, pitching, giants\n",
      "Hockey\n",
      "hockey, nhl, playoff, mask, wings, detroit, leafs, the nhl, playoffs, roger\n"
     ]
    }
   ],
   "source": [
    "W = algo.coef_.shape[1]\n",
    "NUM_WORDS = 10\n",
    "class_2_function = {'Baseball': heapq.nsmallest, 'Hockey': heapq.nlargest}\n",
    "\n",
    "for category, function in class_2_function.items():\n",
    "    topic_words = [\n",
    "        num_2_words[w_num]\n",
    "        for w_num in function(NUM_WORDS, range(W), key=lambda w: algo.coef_[0, w])\n",
    "    ]\n",
    "    print(category)\n",
    "    print(', '.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценим качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.fit(dense_feature_matrix, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.9849624060150376\n",
      "Test accuracy 0.8605527638190955\n"
     ]
    }
   ],
   "source": [
    "print('Train accuracy', accuracy_score(algo.predict(dense_feature_matrix), train_data.target))\n",
    "print('Test accuracy', accuracy_score(algo.predict(count_vectorizer.transform(test_data.data)), test_data.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим l1 - регуляризатор с коэффициентом 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79166667 0.75       0.79079498 0.80334728 0.79497908]\n",
      "0.78615760111576\n"
     ]
    }
   ],
   "source": [
    "algo = LogisticRegression(penalty='l1', C=0.1)\n",
    "arr = cross_val_score(algo, dense_feature_matrix, train_data.target, cv=5, scoring='accuracy')\n",
    "print(arr)\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.fit(sparse_feature_matrix, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.8412698412698413\n",
      "Test accuracy 0.7864321608040201\n"
     ]
    }
   ],
   "source": [
    "print('Train accuracy', accuracy_score(algo.predict(dense_feature_matrix), train_data.target))\n",
    "print('Test accuracy', accuracy_score(algo.predict(count_vectorizer.transform(test_data.data)), test_data.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление регуляризатора уменьшает отличие на трейне и тесте, но ухудшает качество."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы не делать векторизацию и обучение раздельно, есть класс Pipeline. Он позволяет объединить в цепочку последовательность действий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"vectorizer\", CountVectorizer(min_df=3, ngram_range=(1, 2))),\n",
    "                     (\"algo\", LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "       ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(train_data.data, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.9849624060150376\n",
      "Test accuracy 0.8605527638190955\n"
     ]
    }
   ],
   "source": [
    "print('Train accuracy', accuracy_score(pipeline.predict(train_data.data), train_data.target))\n",
    "print('Test accuracy', accuracy_score(pipeline.predict(test_data.data), test_data.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения примерно такие же как мы получали ранее, делая шаги раздельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При кроссвалидации нужно, чтобы CountVectorizer не обучался на тесте. Pipeline позволяет это сделать просто."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86666667 0.8875     0.87866109 0.83682008 0.84937238]\n",
      "0.8638040446304045\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(CountVectorizer(min_df=3, ngram_range=(1, 2)),\n",
    "                         LogisticRegression())\n",
    "arr = cross_val_score(pipeline, train_data.data, train_data.target, cv=5, scoring='accuracy')\n",
    "print(arr)\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84711779 0.85964912 0.85463659]\n",
      "0.8538011695906432\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(CountVectorizer(min_df=3, ngram_range=(1, 2)),\n",
    "                         LogisticRegression())\n",
    "arr = cross_val_score(pipeline, train_data.data, train_data.target, cv=3, scoring='accuracy')\n",
    "print(arr)\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Pipeline можно добавлять новые шаги препроцессинга данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее про tf-idf можно прочитать здесь: https://ru.wikipedia.org/wiki/TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.925      0.92083333 0.89121339 0.86610879 0.88702929]\n",
      "0.898036959553696\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(CountVectorizer(min_df=3, ngram_range=(1, 2)),\n",
    "                         TfidfTransformer(),\n",
    "                         LogisticRegression())\n",
    "arr = cross_val_score(pipeline, train_data.data, train_data.target, cv=5, scoring='accuracy')\n",
    "print(arr)\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(train_data.data, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.9807852965747702\n",
      "Test accuracy 0.8944723618090452\n"
     ]
    }
   ],
   "source": [
    "print('Train accuracy', accuracy_score(pipeline.predict(train_data.data), train_data.target))\n",
    "print('Test accuracy', accuracy_score(pipeline.predict(test_data.data), test_data.target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
