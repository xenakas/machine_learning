{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализуем методы для наивного байеса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем выборку, в которой каждый признак имеет некоторое своё распределение, параметры которого отличаются для каждого класса. Затем реализуем несколько методов для класса, который уже частично написан ниже:\n",
    "- метод predict\n",
    "- метод \\_find\\_expon\\_params и \\_get\\_expon\\_density для экспоненциального распределения\n",
    "- метод \\_find\\_norm\\_params и \\_get\\_norm\\_probability для биномиального распределения\n",
    "\n",
    "Для имплементации \\_find\\_something\\_params изучите документацию функций для работы с этими распределениями в scipy.stats и используйте предоставленные там методы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем параметры генерации для трех датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 3), (5000,), ['bernoulli', 'expon', 'norm'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_params_set0 = [(scipy.stats.bernoulli, [dict(p=0.1), dict(p=0.5)]),\n",
    "                   ]\n",
    "\n",
    "func_params_set1 = [(scipy.stats.bernoulli, [dict(p=0.1), dict(p=0.5)]),\n",
    "                    (scipy.stats.expon, [dict(scale=1), dict(scale=0.3)]),\n",
    "                   ]\n",
    "\n",
    "func_params_set2 = [(scipy.stats.bernoulli, [dict(p=0.1), dict(p=0.5)]),\n",
    "                    (scipy.stats.expon, [dict(scale=1), dict(scale=0.3)]),\n",
    "                    (scipy.stats.norm, [dict(loc=0, scale=1), dict(loc=1, scale=2)]),\n",
    "                   ]\n",
    "\n",
    "def generate_dataset_for_nb(func_params_set=[], size = 2500, random_seed=0):\n",
    "    '''\n",
    "    Генерирует выборку с заданными параметрами распределений P(x|y).\n",
    "    Число классов задается длиной списка с параметрами.\n",
    "    Возвращает X, y, список с названиями распределений\n",
    "    '''\n",
    "    np.random.seed(random_seed) \n",
    "\n",
    "    X = []\n",
    "    names = []\n",
    "    \n",
    "    for func, params in func_params_set:\n",
    "        names.append(func.name) \n",
    "        f = []\n",
    "        for i, param in enumerate(params):\n",
    "            f.append(func.rvs(size=size, **param)) #  *args is used to send a non-keyworded variable length argument list to the function. **kwargs allows you to pass keyworded variable length of arguments to a function. \n",
    "        f = np.concatenate(f).reshape(-1,1) # New shape as (-1, 2). row unknown, column 2. \n",
    "        X.append(f)\n",
    "\n",
    "    X = np.concatenate(X, 1) # half distributed with first params, half with second\n",
    "    \n",
    "    y = np.array([0] * size + [1] * size) # half first class, half second\n",
    "\n",
    "    #shuffle_inds = np.random.choice(range(len(X)), size=len(X), replace=False)\n",
    "    #X = X[shuffle_inds]\n",
    "    #y = y[shuffle_inds]\n",
    "\n",
    "    return X, y, names \n",
    "\n",
    "X, y, distrubution_names = generate_dataset_for_nb(func_params_set2)\n",
    "X.shape, y.shape, distrubution_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.45693771, -0.72582032,  0.        ],\n",
       "       [ 0.        ,  0.35328666,  0.56347552,  0.        ],\n",
       "       [ 0.        ,  0.07106601, -0.43563209,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.4261782 ,  3.01704945,  1.        ],\n",
       "       [ 0.        ,  0.00722247,  2.52513833,  1.        ],\n",
       "       [ 1.        ,  0.50391801, -0.37623252,  1.        ]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((X,np.array([y]).T), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14016722, 0.19534797, 0.15659212, ..., 0.11711708, 0.14633245,\n",
       "       0.15977091])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.bernoulli.pmf(X[:,0], np.mean(X[2500:,1]))\n",
    "scipy.stats.expon.pdf(X[:,1], scale=np.mean(X[2500:,1]))\n",
    "scipy.stats.norm.pdf(X[:,2], loc=np.mean(X[2500:,2]), scale=np.std(X[2500:,2])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class NaiveBayes(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    Реализация наивного байеса, которая помимо X, y\n",
    "    принимает на вход во время обучения \n",
    "    виды распределений значений признаков\n",
    "    '''\n",
    "    def __init__(self):  # For example, __file__ indicates the location of Python file, __eq__ is executed when a == b expression is excuted. \n",
    "        pass # class C: pass   -  a class with no methods (yet)\n",
    "    \n",
    "    # bernoulli\n",
    "    \n",
    "    def _find_bernoulli_params(self, x):           #x, *_, y = (1, 2, 3, 4, 5) # x = 1, y = 5 \n",
    "        '''\n",
    "        метод возвращает найденный параметр `p`\n",
    "        распределения scipy.stats.bernoulli\n",
    "        '''\n",
    "        return dict(p=np.mean(x))\n",
    "                                                     # single_trailing_underscore_: This convention could be used for avoiding conflict with Python keywords or built-ins. You might not use it often.\n",
    "    def _get_bernoulli_probability(self, x, params): # _single_leading_underscore: This convention is used for declaring private variables, functions, methods and classes in a module. Anything with this convention are ignored in from module import *. \n",
    "        '''\n",
    "        метод возвращает вероятность x для данных\n",
    "        параметров распределния\n",
    "        '''\n",
    "        return scipy.stats.bernoulli.pmf(x, **params) # Probability mass function.\n",
    "\n",
    "    # exp\n",
    "    \n",
    "    def _find_expon_params(self, x):\n",
    "        '''\n",
    "        метод возвращает ML - оценку параметра `scale` \n",
    "        распределения scipy.stats.expon\n",
    "        '''\n",
    "        return dict(scale=np.mean(x)) \n",
    "    \n",
    "    def _get_expon_density(self, x, params):\n",
    "        '''\n",
    "        метод возвращает плотность распределения x для данных\n",
    "        параметров распределния\n",
    "        '''\n",
    "        return scipy.stats.expon.pdf(x, **params) # Probability density function.\n",
    "\n",
    "    \n",
    "    #norm\n",
    "\n",
    "    def _find_norm_params(self, x):\n",
    "        '''\n",
    "        метод возвращает ML - оценку параметра `loc`, `scale` \n",
    "        распределения scipy.stats.norm\n",
    "        '''\n",
    "        return dict(loc=np.mean(x), scale=np.std(x))    \n",
    "    \n",
    "    def _get_norm_density(self, x, params):\n",
    "        '''\n",
    "        метод возвращает плотность распределения x для данных\n",
    "        параметров распределния\n",
    "        '''\n",
    "        return scipy.stats.norm.pdf(x, **params) # Probability density function.   \n",
    "\n",
    "    # get get\n",
    "    \n",
    "    def _get_params(self, x, distribution):\n",
    "        '''\n",
    "        x - значения из распределения,\n",
    "        distribution - название распределения в scipy.stats\n",
    "        '''\n",
    "        if distribution == 'bernoulli':\n",
    "            return self._find_bernoulli_params(x)\n",
    "        elif distribution == 'expon':\n",
    "            return self._find_expon_params(x)\n",
    "        elif distribution == 'norm':\n",
    "            return self._find_norm_params(x)\n",
    "        else:\n",
    "            raise NotImplementedError('Unknown distribution')\n",
    "            \n",
    "    def _get_probability_or_density(self, x, distribution, params):\n",
    "        '''\n",
    "        x - значения,\n",
    "        distribytion - название распределения в scipy.stats,\n",
    "        params - параметры распределения\n",
    "        '''\n",
    "        if distribution == 'bernoulli':\n",
    "            return self._get_bernoulli_probability(x, params)\n",
    "        elif distribution == 'expon':\n",
    "            return self._get_expon_density(x, params)\n",
    "        elif distribution == 'norm':\n",
    "            return self._get_norm_density(x, params)\n",
    "        else:\n",
    "            raise NotImplementedError('Unknown distribution')\n",
    "\n",
    "    # fit predict        \n",
    "            \n",
    "    def fit(self, X, y, distrubution_names):\n",
    "        '''\n",
    "        X - обучающая выборка,\n",
    "        y - целевая переменная,\n",
    "        feature_distributions - список названий распределений, \n",
    "        по которым предположительно распределны значения P(x|y)\n",
    "        ''' \n",
    "        assert X.shape[1] == len(distrubution_names)\n",
    "        assert set(y) == {0, 1}\n",
    "        self.n_classes = len(np.unique(y)) # 2 classes\n",
    "        self.distrubution_names = distrubution_names \n",
    "        \n",
    "        self.y_prior = [(y == j).mean() for j in range(self.n_classes)]   # [0.5, 0.5]\n",
    "        \n",
    "        self.distributions_params = defaultdict(dict) # defaultdict(dict, {})\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            distribution = self.distrubution_names[i]\n",
    "            for j in range(self.n_classes):\n",
    "                values = X[y == j, i]\n",
    "                self.distributions_params[j][i] = \\\n",
    "                    self._get_params(values, distribution)\n",
    "        \n",
    "        return self.distributions_params\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        X - тестовая выборка\n",
    "        '''\n",
    "        assert X.shape[1] == len(self.distrubution_names)\n",
    "                \n",
    "        preds = []\n",
    "            \n",
    "        for i in range(X.shape[0]):\n",
    "            \n",
    "            poster = np.log(np.array(self.y_prior))  # [0.5, 0.5]\n",
    "             \n",
    "            for k in range(len(self.distrubution_names)): \n",
    "                \n",
    "                proba = []\n",
    "                \n",
    "                for j in range(self.n_classes): \n",
    "                \n",
    "                    pr = self._get_probability_or_density([X[i][k]], self.distrubution_names[k], self.distributions_params[j][k])\n",
    "                    \n",
    "                    proba.extend(pr)\n",
    "                    \n",
    "                poster = np.array(poster) + np.log(np.array(proba)) # [0.3, 0.7]\n",
    "            \n",
    "            preds.append(np.argmax(poster))\n",
    "        \n",
    "        \n",
    "        # нужно реализовать подсчет аргмаксной формулы, по которой \n",
    "        # наивный байес принимает решение о принадлежности объекта классу\n",
    "        # и применить её для каждого объекта в X\n",
    "        \n",
    "        # примечание: обычно подсчет этой формулы реализуют через \n",
    "        # её логарифмирование, то есть, через сумму логарифмов вероятностей, \n",
    "        # поскольку перемножение достаточно малых вероятностей будет вести\n",
    "        # к вычислительным неточностям\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим результат на примере первого распределения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict, {0: {0: {'p': 0.1128}}, 1: {0: {'p': 0.482}}})"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y, distrubution_names = generate_dataset_for_nb(func_params_set0)\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.fit(X, y, ['bernoulli'])\n",
    "\n",
    "# defaultdict(dict, {0: {0: {'p': 0.1128}}, 1: {0: {'p': 0.482}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score  # The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is :  F1 = 2 * (precision * recall) / (precision + recall),  where preision = true positives/positives; recall = true poisitives/relevant (relevant = true positives + false negatives) ) \n",
    "\n",
    "prediction = nb.predict(X)\n",
    "score = f1_score(y, prediction)\n",
    "print('{:.4f}'.format(score))\n",
    "\n",
    "# 0.6045"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/File:Precisionrecall.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {0: {0: {'p': 0.1128},\n",
       "              1: {'scale': 0.9852643613643739},\n",
       "              2: {'loc': -0.031953073806878646, 'scale': 0.9861590182151528}},\n",
       "             1: {0: {'p': 0.482},\n",
       "              1: {'scale': 0.2960380256457503},\n",
       "              2: {'loc': 0.9528354019954841, 'scale': 2.004030301628345}}})"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y, distrubution_names = generate_dataset_for_nb(func_params_set2)\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.fit(X, y, ['bernoulli', 'expon', 'norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7890\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score  # The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is :  F1 = 2 * (precision * recall) / (precision + recall),  where preision = true positives/positives; recall = true poisitives/relevant (relevant = true positives + false negatives) ) \n",
    "\n",
    "prediction = nb.predict(X)\n",
    "score = f1_score(y, prediction)\n",
    "print('{:.4f}'.format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ответы для формы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответом для формы должны служить числа, которые будут выведены ниже. Все ответы проверены: в этих примерах получается одинаковый результат и через сумму логарифмов, и через произведение вероятностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7615\n",
      "0.7511\n",
      "0.7890\n"
     ]
    }
   ],
   "source": [
    "scipy.stats.bernoulli.name\n",
    "\n",
    "for fps in (func_params_set0 * 2,\n",
    "            func_params_set1, \n",
    "            func_params_set2):\n",
    "    \n",
    "\n",
    "    X, y, distrubution_names = generate_dataset_for_nb(fps)\n",
    "    \n",
    "    nb = NaiveBayes()\n",
    "    nb.fit(X, y, distrubution_names)\n",
    "    prediction = nb.predict(X)\n",
    "    score = f1_score(y, prediction)\n",
    "    print('{:.4f}'.format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
