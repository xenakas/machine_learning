{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бэггинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $\\xi_1, \\xi_2, \\dots, \\xi_n$ - одинаково распределённые величины, скоррелированные с коэффициентом корреляции $\\rho$ и дисперсией $\\sigma^2$. \n",
    "\n",
    "$$ Var ( \\bar{\\xi} )  =  Var  \\left(\\frac1n \\sum_{i=1}^n \\xi_i  \\right) = \\frac1{n^2} cov (\\sum_{i=1}^n \\xi_i, \\sum_{i=1}^n \\xi_i) = \\frac1{n^2} \\sum_{i=1, j=1}^n cov(\\xi_i, \\xi_j) =$$ $$ = \\frac1{n^2} \\sum_{i=1}^n var(\\xi_i) + \\frac1{n^2} \\sum_{i=1, j=1, i\\neq j}^n cov (\\xi_i, \\xi_j) = \\frac1{n^2} \\sum_{i=1}^n \\sigma^2+ \\frac1{n^2} \\sum_{i=1, j=1, i\\neq j}^n \\rho \\sigma^2 =$$\n",
    "$$ = \\frac1{n^2} n \\sigma^2 + \\frac1{n^2} n(n-1) \\rho \\sigma^2  = \\frac{\\sigma^2( 1 + \\rho(n-1))}{n}$$\n",
    "\n",
    "Таким образом, чем менее скоррелированы между собой величины, тем меньше дисперсия их среднего. \n",
    "\n",
    "Грубо говоря в этом и состоит идея бэггинга: \n",
    "сделать много максимально независимых моделей, усреднить из и получить более устойчивые предсказания "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бэггинг над решающими деревьями\n",
    "\n",
    "Посмотрим, какие модели можно получить из деревьев с помощью их рандомизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('hr_kaggle.csv')\n",
    "\n",
    "target = 'left'\n",
    "features = [c for c in data if c != target]\n",
    "\n",
    "X, y = data[features], data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_d3 = DecisionTreeClassifier(max_features=int(len(features) ** 0.5)) # Решающее дерево с рандомизацией в сплитах\n",
    "d3 = DecisionTreeClassifier() # Обычное решающее дерево"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество классификации решающим деревом с настройками по-умолчанию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree: 0.65137624858305\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision tree:\", cross_val_score(d3, X, y).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бэггинг над решающими деревьями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D3 bagging: 0.7174495299059812\n"
     ]
    }
   ],
   "source": [
    "print(\"D3 bagging:\", cross_val_score(BaggingClassifier(d3, random_state=42), X, y).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Усредненная модель оказалась намного лучше: \n",
    "у решающих деревьев есть существенный недостаток - нестабильность получаемого дерева при небольших изменениях в выборке, но бэггинг обращает этот недостаток в достоинство, ведь усредненная модель работает лучше, когда базовые модели слабо скоррелированы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучив параметры DecisionTreeClassifier, можно найти хороший способ сделать деревья еще более различными - при построении каждого узла отбирать случайные max_features признаков и искать информативное разбиение только по одному из них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized decision tree: 0.665644448889778\n"
     ]
    }
   ],
   "source": [
    "print(\"Randomized decision tree:\", cross_val_score(rnd_d3, X, y).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized D3 Bagging: 0.7190487697539508\n"
     ]
    }
   ],
   "source": [
    "print(\"Randomized D3 Bagging:\", cross_val_score(BaggingClassifier(rnd_d3), X, y).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В среднем, качество получается еще лучше. Для выбора числа признаков использовалась часто применяемая на практике эвристика - брать корень из общего числа признаков. Если бы мы решали задачу регрессии - брали бы треть от общего числа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторные запуски cross_val_score будут показывать различное качество модели.\n",
    "Это зависит от параметра рандомизации модели \"random_state\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### BaggingClassifier vs RandomForestClassifier():\n",
    "\n",
    "Bagging improves variance by averaging/majority selection of outcome from multiple fully grown trees on variants of training set. It uses Bootstrap with replacement to generate multiple training sets.\n",
    "\n",
    "Bagging has a single parameter, which is the number of trees. All trees are fully grown binary tree (unpruned) and at each node in the tree one searches over **all** features to find the feature that best splits the data at that node.\n",
    "\n",
    "Random Forests improve variance by reducing correlation between trees, this is accomplished by random selection of feature-subset for split at each node. \n",
    "\n",
    "Random forests has 2 parameters: the number of trees (the same as bagging) and  $mtry$ (unique to randomforests)   which is how many features to search over to find the best feature. This parameter is usually $1/3*D$ for regression and $\\sqrt{D}$ for classification. Thus during tree creation randomly $mtry$ number of features are chosen from all available features and the best feature that splits the data is chosen.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized D3 Bagging: 0.7194494632259785\n"
     ]
    }
   ],
   "source": [
    "print(\"Randomized D3 Bagging:\", cross_val_score(BaggingClassifier(rnd_d3, random_state=42), X, y).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.7232495965859839\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest:\", cross_val_score(RandomForestClassifier(random_state=42), X, y).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.6287053143962126\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression:\", cross_val_score(LogisticRegression(), X, y).mean())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
